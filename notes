framebuffer-draw

Cubemaps: reflective surfaces

https://learnopengl.com/Advanced-OpenGL/Cubemaps
this uses framebuffer cubes.

There is a huge difference between capturing the framebuffer vs drawing the quad with the framebuffer as the texture.
For both parts, iterate through all the framebuffers, but iterate through each step separately. 
We want to create two Framebuffer objects within the vector, and then we want to draw both of them onto the screen.
- this is because capturing the framebuffer just captures whatever is currently set to be rendered.
- capturing the framebuffer is capturing the raw world, we want to make sure we capture the view of the world before we create the textured quad with the framebuffer

Next step: we want a camera that can have its own direction(it can point backwards, for example)
- Add a new function to the Camera, SetCameraEyeDirection
- Add a second camera to Renderer
- Set the direction of the second camera to be backwards, and update input to apply input to the second camera too in SDLGraphicsProgram

Updating models based on camera position occurs in SceneNodes, (WE DO NOT NEED TO COPY SINCE THE SHADER UNIFORMS CAN EASILY BE REMADE FROM ANOTHER CAMERA'S PERSPECTIVE, USING UPDATE IN SCENENODE UPDATES THE SHADER UNIFORMS, NOT THE OBJECT DATA AND COORDINATES ITSELF)
- When we call Update using another camera, we simply overwrite the previous shader uniforms from a previous camera. The previous shader's data is recoverable as long as the previous camera is used to call Update again.
- All update does is fetch MVP matrices and camera data into the shader uniforms passed into OpenGL for calculation, it doesn't actually calculate anything without OpenGL. 
- Then, make sure that there is a field within the FrameBuffer that corresponds to the ID of the camera that it belongs to(Since we are going to use multiple cameras)
- And then call Update on the root SceneNode for each FrameBuffer, this is very computationally expensive but it is what we have to do.
    - Maybe eventually we could consider generating one framebuffer for each camera, rather than one for each quad.

Possible approach to rendering a mirror:
- Create a new child Object class called Mirror
- For the first pass, the texture could just be a satring color/texture when rendering the quad
    - Then, after capturing the view from the quad's camera, store the view somewhere and use that for any future renderings for the quad 
- Depth solution, to create the effect of continuous mirrors, we could probably continuously render the view from the mirror for x amounts of passes
    - This is how we can achieve the warp effect from mirrors

For our Mirror, we just use a regular Textured Quad, we use the inherited function from Object.hpp
- We will not have to set up a Screen quad
- We also will not have to modify Render() since Render() uses a one size fits all Bind() call to bind both the vertices and the texture

SceneNode's localTransform does nothing, I had to impement my own transformation to convert local space to world space if there is a parent

Rear view camera implemented: the key observation I made is that camera direction uses a bounding box within (1,1,1) to determine where the camera is facing
- This does not use roll/pitch/yaw, this is cartesian
- Now, Camera 1(which faces backwards from Camera 0) just outright copies the direction and position coordinates of Camera 0 and inverts them
    - trying to manage them independently earlier was frustrating, and with the implementation of how the camera works, it is not viable.
    - since MoveForward is relative to the camera's direction, I have to copy position from Camera 0 to Camera 1

Current big issue, rotate in Textured Quads does nothing, well actaully it compresses the texture, but fails to actually rotate the corners
- this doesn't matter I guess, this is an issue for another day.